
# File ingestion and schema validation

### Purpose

This is the week 6 project for the Data Glacier Virtual Internship, Data Ingestion, which explores methods of reading large data files of size 2GB+, performs basic validation techniques on the data columns with YAML file, then creates a text file in gzip format, and data summary. 

### Introduction

Data was found in Kaggle datasets, HERE and downloaded to my local Windows 11 computer. The Jupyter Notebook was run using Visual Studio Code and used Python packages that included Pandas, Dask, YAML, tracemalloc, shutil, GZIP, and, CSV. A block of the Glacier Data practice code was used in the val_dat_col() function for printing out file validation messages. The version control used was GIT, along with repositories in GitLab and GitHub. 

### Data

NYC Parking Ticket Data

Go to [Kaggle](https://www.kaggle.com/datasets/new-york-city/nyc-parking-tickets)


### Authors and acknowledgment
Credit to Data Glacier for a chunck of code in my column validation function.

### License
Open Source - MIT

### Project status
Project is complete for the Data Glacier assignment.

